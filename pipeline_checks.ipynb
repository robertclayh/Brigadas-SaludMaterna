{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 2,457 rows × 17 columns from out/adm2_risk_daily.csv\n",
      "\n",
      "Columns: ['run_date', 'adm1_name', 'adm2_name', 'adm2_code', 'pop_wra', 'w_exposure', 'v30', 'v3m', 'dlt_v30_raw', 'spillover', 'cast_state', 'access_A', 'strain_H', 'mvi', 'DCR100', 'PRS100', 'priority100']\n",
      "\n",
      "Missing values per column:\n",
      "run_date       0\n",
      "spillover      0\n",
      "PRS100         0\n",
      "DCR100         0\n",
      "mvi            0\n",
      "strain_H       0\n",
      "access_A       0\n",
      "cast_state     0\n",
      "dlt_v30_raw    0\n",
      "adm1_name      0\n",
      "v3m            0\n",
      "v30            0\n",
      "w_exposure     0\n",
      "pop_wra        0\n",
      "adm2_code      0\n",
      "adm2_name      0\n",
      "priority100    0\n",
      "dtype: int64\n",
      "\n",
      "⚠️ Columns that are entirely zero: []\n",
      "⚠️ Columns with constant values: []\n",
      "\n",
      "Non-zero rate by column:\n",
      "pop_wra        0.999\n",
      "w_exposure     1.000\n",
      "v30            0.058\n",
      "v3m            0.185\n",
      "dlt_v30_raw    0.041\n",
      "spillover      0.280\n",
      "cast_state     0.924\n",
      "access_A       0.868\n",
      "strain_H       0.950\n",
      "mvi            0.950\n",
      "DCR100         0.910\n",
      "PRS100         0.913\n",
      "priority100    0.913\n",
      "dtype: float64\n",
      "\n",
      "DCR100 range: 0.000 to 164.166, mean=32.379, nonzero%=91.0\n",
      "\n",
      "PRS100 range: 0.000 to 144.633, mean=20.851, nonzero%=91.3\n",
      "\n",
      "priority100 range: 0.000 to 152.446, mean=25.462, nonzero%=91.3\n",
      "\n",
      "access_A range: 0.000 to 1.000, mean=0.326, nonzero%=86.8\n",
      "\n",
      "strain_H range: 0.000 to 1.000, mean=0.363, nonzero%=95.0\n",
      "\n",
      "mvi range: 0.000 to 1.000, mean=0.538, nonzero%=95.0\n",
      "\n",
      "cast_state range: 0.000 to 1.000, mean=0.306, nonzero%=92.4\n",
      "\n",
      "Checking for ADM1 with all-zero values in key metrics...\n",
      "✓ All ADM1 regions have non-zero values for at least one row in each key metric.\n",
      "\n",
      "Top correlations among key metrics:\n",
      "             DCR100  PRS100  priority100  access_A  strain_H   mvi  cast_state\n",
      "DCR100         1.00    0.81         0.94      0.67      0.61 -0.09        0.24\n",
      "PRS100         0.81    1.00         0.96      0.50      0.44 -0.22        0.48\n",
      "priority100    0.94    0.96         1.00      0.61      0.55 -0.17        0.39\n",
      "access_A       0.67    0.50         0.61      1.00      0.94 -0.08        0.14\n",
      "strain_H       0.61    0.44         0.55      0.94      1.00 -0.03        0.11\n",
      "mvi           -0.09   -0.22        -0.17     -0.08     -0.03  1.00       -0.14\n",
      "cast_state     0.24    0.48         0.39      0.14      0.11 -0.14        1.00\n",
      "\n",
      "Validation complete.\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "# Sanity / validation checks for adm2_risk_daily.csv\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "OUT_DIR = Path(\"out\")\n",
    "CSV_PATH = OUT_DIR / \"adm2_risk_daily.csv\"\n",
    "\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print(f\"Loaded {len(df):,} rows × {df.shape[1]} columns from {CSV_PATH}\\n\")\n",
    "\n",
    "# --- Basic structure and completeness ---\n",
    "print(\"Columns:\", list(df.columns))\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(df.isna().sum().sort_values(ascending=False))\n",
    "\n",
    "# --- Numeric coverage and all-zero detection ---\n",
    "num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "zero_cols = [c for c in num_cols if (df[c].fillna(0) == 0).all()]\n",
    "const_cols = [c for c in num_cols if df[c].nunique(dropna=True) <= 1]\n",
    "\n",
    "print(\"\\n⚠️ Columns that are entirely zero:\", zero_cols)\n",
    "print(\"⚠️ Columns with constant values:\", const_cols)\n",
    "\n",
    "# --- Non-zero rate (fraction of rows with any value > 0) ---\n",
    "nz_rate = (df[num_cols] > 0).sum() / len(df)\n",
    "print(\"\\nNon-zero rate by column:\")\n",
    "print(nz_rate.round(3))\n",
    "\n",
    "# --- Basic numeric ranges for main indices ---\n",
    "key_cols = [\"DCR100\", \"PRS100\", \"priority100\", \"access_A\", \"strain_H\", \"mvi\", \"cast_state\"]\n",
    "for c in key_cols:\n",
    "    if c in df.columns:\n",
    "        print(f\"\\n{c} range: {df[c].min():.3f} to {df[c].max():.3f}, \"\n",
    "              f\"mean={df[c].mean():.3f}, nonzero%={(df[c] > 0).mean() * 100:.1f}\")\n",
    "\n",
    "# --- ADM1-level zero checks (e.g., ensure no entire state is zero) ---\n",
    "group_keys = [\"adm1_name\"]\n",
    "check_cols = [\"facilities\", \"pop_wra\", \"DCR100\", \"PRS100\", \"priority100\", \"access_A\", \"strain_H\", \"mvi\"]\n",
    "\n",
    "print(\"\\nChecking for ADM1 with all-zero values in key metrics...\")\n",
    "if not all(col in df.columns for col in group_keys):\n",
    "    print(\"[Skip] ADM1 check — missing adm1_name column.\")\n",
    "else:\n",
    "    zero_report = []\n",
    "    for adm1, g in df.groupby(\"adm1_name\"):\n",
    "        for c in [col for col in check_cols if col in g.columns]:\n",
    "            if (g[c].fillna(0) == 0).all():\n",
    "                zero_report.append((adm1, c))\n",
    "    if zero_report:\n",
    "        print(\"⚠️ ADM1-level all-zero metrics detected:\")\n",
    "        for adm1, c in zero_report:\n",
    "            print(f\"  - {adm1}: all zeros in {c}\")\n",
    "    else:\n",
    "        print(\"✓ All ADM1 regions have non-zero values for at least one row in each key metric.\")\n",
    "\n",
    "# --- Sanity thresholds / warnings ---\n",
    "if len(zero_cols) > 0:\n",
    "    print(\"\\n[Warning] Some numeric columns are all zeros; check upstream calculations.\")\n",
    "if len(df) == 0:\n",
    "    print(\"\\n[Warning] Empty CSV — pipeline may have filtered everything out.\")\n",
    "if \"priority100\" in df.columns and df[\"priority100\"].max() <= 0:\n",
    "    print(\"\\n[Warning] priority100 has no positive values (possible normalization issue).\")\n",
    "\n",
    "# --- Optional: quick correlation sanity check (should have some variability) ---\n",
    "if len(df) > 5:\n",
    "    corr = df[num_cols].corr()\n",
    "    print(\"\\nTop correlations among key metrics:\")\n",
    "    print(corr.loc[[c for c in key_cols if c in corr.index], [c for c in key_cols if c in corr.columns]].round(2))\n",
    "\n",
    "print(\"\\nValidation complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading ADM2 shapefile…\n",
      "ADM2 polygons: 2,457 rows, EPSG:4326\n",
      "\n",
      "Checking CAST (state-level)…\n",
      "CAST coverage: matched 98.6% of ADM2 rows (by state).\n",
      "States with missing CAST matches:\n",
      "           adm1_name adm1_code\n",
      "    Distrito Federal      MX09\n",
      "Querétaro de Arteaga      MX22\n",
      "\n",
      "Checking CONEVAL municipal poverty (ADM2)…\n",
      "CONEVAL coverage: 2,457 / 2,457 ADM2 codes matched (0 missing, 12 extra).\n",
      "poverty_rate describe():\n",
      "count    2466.000000\n",
      "mean       62.002065\n",
      "std        21.903723\n",
      "min         5.450951\n",
      "25%        45.580691\n",
      "50%        62.745101\n",
      "75%        80.316135\n",
      "max        99.646676\n",
      "Name: poverty_rate, dtype: float64\n",
      "\n",
      "Checking CLUES facility counts (ADM2)…\n",
      "CLUES coverage: 932 / 2,457 ADM2 codes matched (1525 missing, 6 extra).\n",
      "facilities describe():\n",
      "count    938.000000\n",
      "mean       6.152452\n",
      "std       12.040008\n",
      "min        1.000000\n",
      "25%        1.000000\n",
      "50%        2.000000\n",
      "75%        6.000000\n",
      "max      135.000000\n",
      "Name: facilities, dtype: float64\n",
      "\n",
      "Checking Population (ADM2)…\n",
      "Population coverage: 2,457 / 2,457 ADM2 codes matched (0 missing, 0 extra).\n",
      "pop_total describe():\n",
      "count    2.457000e+03\n",
      "mean     5.365294e+04\n",
      "std      1.538357e+05\n",
      "min      0.000000e+00\n",
      "25%      4.681000e+03\n",
      "50%      1.426500e+04\n",
      "75%      3.788100e+04\n",
      "max      2.010930e+06\n",
      "Name: pop_total, dtype: float64\n",
      "pop_wra describe():\n",
      "count      2457.000000\n",
      "mean      13413.235653\n",
      "std       38458.936944\n",
      "min           0.000000\n",
      "25%        1170.000000\n",
      "50%        3566.000000\n",
      "75%        9470.000000\n",
      "max      502732.000000\n",
      "Name: pop_wra, dtype: float64\n",
      "\n",
      "ACLED events verification (optional)…\n",
      "No ACLED events CSV at data/acled_events_90d.csv; skipping ACLED spatial/name join checks.\n",
      "\n",
      "Checks complete. Reports (if any) are in out/checks/\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# Input Join Checker — ADM2/ADM1 consistency and ACLED spatial/name joins\n",
    "\n",
    "# %%\n",
    "import os\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "import numpy as np\n",
    "from unidecode import unidecode\n",
    "\n",
    "# Optional: spatial join test for ACLED event points if present\n",
    "ACLED_POINTS_CSV = Path(\"data/acled_events_90d.csv\")  # optional; needs latitude, longitude\n",
    "\n",
    "DATA_DIR = Path(\"data\")\n",
    "OUT_DIR  = Path(\"out/checks\")\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Core inputs (as produced/used in your pipeline)\n",
    "ADM2_SHP         = DATA_DIR / \"mex_admbnda_govmex_20210618_SHP\" / \"mex_admbnda_adm2_govmex_20210618.shp\"\n",
    "POP_CSV          = DATA_DIR / \"pop_adm2.csv\"                       # adm2_code, pop_total, pop_wra\n",
    "CLUES_CSV        = DATA_DIR / \"clues_facility_counts_adm2.csv\"     # adm2_code, facilities\n",
    "CONEVAL_CSV      = DATA_DIR / \"coneval_muni.csv\"                    # adm2_code, poverty_rate\n",
    "CAST_STATE_CSV   = DATA_DIR / \"cast_state.csv\"                      # adm1_name, cast_raw (or scaled)\n",
    "\n",
    "# Utility: consistent name normalization (strip accents, case, extra spaces)\n",
    "def norm_name(s: pd.Series) -> pd.Series:\n",
    "    return (\n",
    "        s.fillna(\"\")\n",
    "         .astype(str)\n",
    "         .str.strip()\n",
    "         .str.replace(r\"\\s+\", \" \", regex=True)\n",
    "         .apply(lambda x: unidecode(x).strip().title())\n",
    "    )\n",
    "\n",
    "def as_str_no_nan(x):\n",
    "    return x.fillna(\"\").astype(str)\n",
    "\n",
    "print(\"Loading ADM2 shapefile…\")\n",
    "adm2 = gpd.read_file(ADM2_SHP)\n",
    "# Keep only what we need\n",
    "keep_cols = [\"ADM1_ES\",\"ADM1_PCODE\",\"ADM2_ES\",\"ADM2_PCODE\",\"geometry\"]\n",
    "adm2 = adm2[keep_cols].rename(columns={\n",
    "    \"ADM1_ES\": \"adm1_name\",\n",
    "    \"ADM1_PCODE\": \"adm1_code\",\n",
    "    \"ADM2_ES\": \"adm2_name\",\n",
    "    \"ADM2_PCODE\":\"adm2_code\"\n",
    "}).to_crs(4326)\n",
    "\n",
    "# Normalize readable names (codes remain as-is)\n",
    "adm2[\"adm1_name_norm\"] = norm_name(adm2[\"adm1_name\"])\n",
    "adm2[\"adm2_name_norm\"] = norm_name(adm2[\"adm2_name\"])\n",
    "adm2[\"adm2_code\"]      = as_str_no_nan(adm2[\"adm2_code\"])\n",
    "\n",
    "print(f\"ADM2 polygons: {len(adm2):,} rows, {adm2.crs}\")\n",
    "\n",
    "# --- CAST (ADM1) check ---\n",
    "print(\"\\nChecking CAST (state-level)…\")\n",
    "if CAST_STATE_CSV.exists():\n",
    "    cast = pd.read_csv(CAST_STATE_CSV)\n",
    "    # Support either cast_raw or pre-scaled column\n",
    "    if \"cast_raw\" in cast.columns:\n",
    "        cast[\"cast_state\"] = cast[\"cast_raw\"]\n",
    "    elif \"cast_state\" not in cast.columns:\n",
    "        cast[\"cast_state\"] = np.nan\n",
    "    cast[\"adm1_name_norm\"] = norm_name(cast[\"adm1_name\"])\n",
    "    # Distinct to avoid dupes\n",
    "    cast1 = cast[[\"adm1_name_norm\",\"cast_state\"]].drop_duplicates()\n",
    "\n",
    "    # Left-join CAST to ADM2 by normalized ADM1 name\n",
    "    adm2_cast = adm2.merge(cast1, on=\"adm1_name_norm\", how=\"left\")\n",
    "\n",
    "    # Report coverage\n",
    "    missing_cast = adm2_cast[adm2_cast[\"cast_state\"].isna()][[\"adm1_name\",\"adm1_code\"]].drop_duplicates()\n",
    "    print(f\"CAST coverage: matched {adm2_cast['cast_state'].notna().mean():.1%} of ADM2 rows (by state).\")\n",
    "    if not missing_cast.empty:\n",
    "        print(\"States with missing CAST matches:\")\n",
    "        print(missing_cast.to_string(index=False))\n",
    "        missing_cast.to_csv(OUT_DIR / \"cast_missing_states.csv\", index=False)\n",
    "else:\n",
    "    print(\"CAST file not found; skipping CAST checks.\")\n",
    "\n",
    "# --- CONEVAL (ADM2) check ---\n",
    "print(\"\\nChecking CONEVAL municipal poverty (ADM2)…\")\n",
    "if CONEVAL_CSV.exists():\n",
    "    coneval = pd.read_csv(CONEVAL_CSV, dtype={\"adm2_code\":\"string\"})\n",
    "    coneval[\"adm2_code\"] = as_str_no_nan(coneval[\"adm2_code\"])\n",
    "    # coverage\n",
    "    adm2_codes = set(adm2[\"adm2_code\"])\n",
    "    cv_codes   = set(coneval[\"adm2_code\"])\n",
    "    missing_in_coneval = sorted(adm2_codes - cv_codes)\n",
    "    extra_in_coneval   = sorted(cv_codes - adm2_codes)\n",
    "\n",
    "    print(f\"CONEVAL coverage: {len(cv_codes & adm2_codes):,} / {len(adm2_codes):,} ADM2 codes matched ({len(missing_in_coneval)} missing, {len(extra_in_coneval)} extra).\")\n",
    "    pd.DataFrame({\"adm2_code\": missing_in_coneval}).to_csv(OUT_DIR/\"coneval_missing_adm2.csv\", index=False)\n",
    "    pd.DataFrame({\"adm2_code\": extra_in_coneval}).to_csv(OUT_DIR/\"coneval_extra_adm2.csv\", index=False)\n",
    "\n",
    "    # Quick distribution of poverty_rate\n",
    "    if \"poverty_rate\" in coneval.columns:\n",
    "        desc = coneval[\"poverty_rate\"].describe()\n",
    "        print(\"poverty_rate describe():\")\n",
    "        print(desc)\n",
    "else:\n",
    "    print(\"CONEVAL file not found; skipping CONEVAL checks.\")\n",
    "\n",
    "# --- CLUES (ADM2) check ---\n",
    "print(\"\\nChecking CLUES facility counts (ADM2)…\")\n",
    "if CLUES_CSV.exists():\n",
    "    clues = pd.read_csv(CLUES_CSV, dtype={\"adm2_code\":\"string\"})\n",
    "    clues[\"adm2_code\"] = as_str_no_nan(clues[\"adm2_code\"])\n",
    "\n",
    "    cl_codes = set(clues[\"adm2_code\"])\n",
    "    missing_in_clues = sorted(adm2_codes - cl_codes)\n",
    "    extra_in_clues   = sorted(cl_codes - adm2_codes)\n",
    "\n",
    "    print(f\"CLUES coverage: {len(cl_codes & adm2_codes):,} / {len(adm2_codes):,} ADM2 codes matched ({len(missing_in_clues)} missing, {len(extra_in_clues)} extra).\")\n",
    "    pd.DataFrame({\"adm2_code\": missing_in_clues}).to_csv(OUT_DIR/\"clues_missing_adm2.csv\", index=False)\n",
    "    pd.DataFrame({\"adm2_code\": extra_in_clues}).to_csv(OUT_DIR/\"clues_extra_adm2.csv\", index=False)\n",
    "\n",
    "    # sanity on counts\n",
    "    if \"facilities\" in clues.columns:\n",
    "        print(\"facilities describe():\")\n",
    "        print(clues[\"facilities\"].describe())\n",
    "else:\n",
    "    print(\"CLUES file not found; skipping CLUES checks.\")\n",
    "\n",
    "# --- POP (ADM2) check ---\n",
    "print(\"\\nChecking Population (ADM2)…\")\n",
    "if POP_CSV.exists():\n",
    "    pop = pd.read_csv(POP_CSV, dtype={\"adm2_code\":\"string\"})\n",
    "    pop[\"adm2_code\"] = as_str_no_nan(pop[\"adm2_code\"])\n",
    "    pp_codes = set(pop[\"adm2_code\"])\n",
    "    missing_in_pop = sorted(adm2_codes - pp_codes)\n",
    "    extra_in_pop   = sorted(pp_codes - adm2_codes)\n",
    "\n",
    "    print(f\"Population coverage: {len(pp_codes & adm2_codes):,} / {len(adm2_codes):,} ADM2 codes matched ({len(missing_in_pop)} missing, {len(extra_in_pop)} extra).\")\n",
    "    pd.DataFrame({\"adm2_code\": missing_in_pop}).to_csv(OUT_DIR/\"pop_missing_adm2.csv\", index=False)\n",
    "    pd.DataFrame({\"adm2_code\": extra_in_pop}).to_csv(OUT_DIR/\"pop_extra_adm2.csv\", index=False)\n",
    "\n",
    "    for col in [\"pop_total\",\"pop_wra\"]:\n",
    "        if col in pop.columns:\n",
    "            print(f\"{col} describe():\")\n",
    "            print(pop[col].describe())\n",
    "else:\n",
    "    print(\"Population file not found; skipping POP checks.\")\n",
    "\n",
    "# --- Optional: ACLED points name-join vs spatial-join comparison ---\n",
    "print(\"\\nACLED events verification (optional)…\")\n",
    "if ACLED_POINTS_CSV.exists():\n",
    "    # Expect columns: latitude, longitude; optionally admin1/admin2\n",
    "    events = pd.read_csv(ACLED_POINTS_CSV)\n",
    "    # Basic cleaning\n",
    "    latcol = next((c for c in events.columns if c.lower() == \"latitude\"), None)\n",
    "    loncol = next((c for c in events.columns if c.lower() == \"longitude\"), None)\n",
    "    if not latcol or not loncol:\n",
    "        print(\"ACLED points present but missing latitude/longitude columns; skipping spatial join test.\")\n",
    "    else:\n",
    "        # Spatial join\n",
    "        ev_gdf = gpd.GeoDataFrame(\n",
    "            events.dropna(subset=[latcol, loncol]).copy(),\n",
    "            geometry=gpd.points_from_xy(events[loncol], events[latcol]),\n",
    "            crs=4326\n",
    "        )\n",
    "\n",
    "        # Keep small sample if extremely large\n",
    "        if len(ev_gdf) > 250_000:\n",
    "            ev_gdf = ev_gdf.sample(250_000, random_state=42).copy()\n",
    "            print(f\"Sampled 250,000 events for spatial join speed (from {len(events):,}).\")\n",
    "\n",
    "        ev_in_adm2 = gpd.sjoin(ev_gdf, adm2[[\"adm2_code\",\"adm1_name_norm\",\"adm2_name_norm\",\"geometry\"]],\n",
    "                               how=\"left\", predicate=\"within\").drop(columns=[\"index_right\"])\n",
    "\n",
    "        # Name-join attempt (if admin1/admin2 exist)\n",
    "        a1 = next((c for c in events.columns if c.lower() in {\"admin1\",\"adm1\",\"state\"}), None)\n",
    "        a2 = next((c for c in events.columns if c.lower() in {\"admin2\",\"adm2\",\"municipio\",\"municipality\"}), None)\n",
    "\n",
    "        if a1 and a2:\n",
    "            tmp = ev_in_adm2.copy()\n",
    "            tmp[\"admin1_norm\"] = norm_name(tmp[a1])\n",
    "            tmp[\"admin2_norm\"] = norm_name(tmp[a2])\n",
    "\n",
    "            # merge to ADM2 names to get a code by names\n",
    "            name_join = tmp.merge(\n",
    "                adm2[[\"adm2_code\",\"adm1_name_norm\",\"adm2_name_norm\"]],\n",
    "                left_on=[\"admin1_norm\",\"admin2_norm\"],\n",
    "                right_on=[\"adm1_name_norm\",\"adm2_name_norm\"],\n",
    "                how=\"left\",\n",
    "                suffixes=(\"\",\"_adm\")\n",
    "            )\n",
    "\n",
    "            # compare codes: spatial vs name-based\n",
    "            both = name_join[[\"adm2_code\", \"adm2_code_adm\"]].copy()\n",
    "            both[\"match\"] = both[\"adm2_code\"].fillna(\"\") == both[\"adm2_code_adm\"].fillna(\"\")\n",
    "            rate = both[\"match\"].mean()\n",
    "            mism = both[~both[\"match\"]].head(20)\n",
    "            print(f\"Name vs spatial join agreement: {rate:.1%} (sample of {len(both):,} events)\")\n",
    "            if not mism.empty:\n",
    "                mism.to_csv(OUT_DIR/\"acled_name_vs_spatial_mismatches_sample.csv\", index=False)\n",
    "                print(\"Wrote sample mismatches to out/checks/acled_name_vs_spatial_mismatches_sample.csv\")\n",
    "        else:\n",
    "            print(\"ACLED events lack admin1/admin2 columns; only spatial coverage was checked.\")\n",
    "\n",
    "        # Spatial coverage rate\n",
    "        cov = ev_in_adm2[\"adm2_code\"].notna().mean()\n",
    "        print(f\"Spatial join coverage (events within ADM2 polygons): {cov:.1%} of events\")\n",
    "else:\n",
    "    print(\"No ACLED events CSV at data/acled_events_90d.csv; skipping ACLED spatial/name join checks.\")\n",
    "\n",
    "print(\"\\nChecks complete. Reports (if any) are in out/checks/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requesting Mexico events 2025-09-25 → 2025-10-25\n",
      "No rows returned. Possible causes: recency cap or filters too narrow.\n",
      "Server message: (no message)\n"
     ]
    }
   ],
   "source": [
    "# %%\n",
    "import os\n",
    "import datetime as dt\n",
    "import requests\n",
    "import pandas as pd\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# --- Config ---\n",
    "TOKEN_URL = \"https://acleddata.com/oauth/token\"\n",
    "READ_URL  = \"https://acleddata.com/api/acled/read\"  # default JSON\n",
    "COUNTRY   = \"Mexico\"\n",
    "\n",
    "# --- Auth ---\n",
    "load_dotenv()\n",
    "ACLED_USER = os.getenv(\"ACLED_USER\")\n",
    "ACLED_PASS = os.getenv(\"ACLED_PASS\")\n",
    "assert ACLED_USER and ACLED_PASS, \"Set ACLED_USER and ACLED_PASS in your .env\"\n",
    "\n",
    "tok = requests.post(\n",
    "    TOKEN_URL,\n",
    "    headers={\"Content-Type\": \"application/x-www-form-urlencoded\"},\n",
    "    data={\n",
    "        \"username\": ACLED_USER,\n",
    "        \"password\": ACLED_PASS,\n",
    "        \"grant_type\": \"password\",\n",
    "        \"client_id\": \"acled\",\n",
    "    },\n",
    "    timeout=60,\n",
    ")\n",
    "tok.raise_for_status()\n",
    "access_token = tok.json()[\"access_token\"]\n",
    "\n",
    "# --- Date window: last 30 days ---\n",
    "end = dt.date.today()\n",
    "start = end - dt.timedelta(days=30)\n",
    "print(f\"Requesting {COUNTRY} events {start} → {end}\")\n",
    "\n",
    "# --- Fetch JSON (default) ---\n",
    "params = {\n",
    "    \"country\": COUNTRY,\n",
    "    \"event_date\": f\"{start}|{end}\",\n",
    "    \"event_date_where\": \"BETWEEN\",\n",
    "    \"limit\": 5000,  # pagination not needed for a 30-day window typically\n",
    "}\n",
    "r = requests.get(\n",
    "    READ_URL,\n",
    "    headers={\"Authorization\": f\"Bearer {access_token}\"},\n",
    "    params=params,\n",
    "    timeout=120,\n",
    ")\n",
    "r.raise_for_status()\n",
    "\n",
    "js = r.json()\n",
    "# ACLED responses include a \"status\" field; 200 means OK even if 'data' is empty.\n",
    "status = js.get(\"status\")\n",
    "data = js.get(\"data\", [])\n",
    "\n",
    "if status != 200:\n",
    "    print(f\"ACLED returned status {status}. Full response:\\n{js}\")\n",
    "elif not data:\n",
    "    # Helpful diagnostics when empty\n",
    "    msg = js.get(\"message\") or js.get(\"detail\") or \"(no message)\"\n",
    "    print(\"No rows returned. Possible causes: recency cap or filters too narrow.\")\n",
    "    print(f\"Server message: {msg}\")\n",
    "else:\n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Downloaded {len(df):,} rows; columns: {list(df.columns)[:10]}…\")\n",
    "    out = \"acled_mexico_30d.csv\"\n",
    "    df.to_csv(out, index=False)\n",
    "    print(f\"Saved → {out}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
